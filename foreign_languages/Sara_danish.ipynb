{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exploring Foreign Languages\n",
    "\n",
    "So far, we have been learning about general ways to explore texts through manipulating strings and regular expressions. Today, we will be focusing on what we can do when texts are in languages other than English. This will just be an introduction to some of the many different modules that can be used for these tasks. The goal is to learn some tools, including Polyglot and translation, that can be jumping off points to see what you may or may not need going forward.\n",
    "\n",
    "### Lesson Outline:\n",
    "- Q&A about what we've gone over so far\n",
    "- Examples (with Sara's data)\n",
    "- Practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installations\n",
    "Uncomment and run the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install translation\n",
    "#!pip install py-translate\n",
    "#!pip install morfessor\n",
    "#!pip install polyglot\n",
    "#!pip install pycld2\n",
    "#!brew install intltool icu4c gettext\n",
    "#!brew link icu4c gettext --force\n",
    "#!CFLAGS=-I/usr/local/opt/icu4c/include LDFLAGS=-L/usr/local/opt/icu4c/lib pip3 install pyicu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "with codecs.open('Skyggebilleder af en Reise til Harzen.txt', 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    read_text = f.read()\n",
    "read_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pulling out a subsection of text for our examples\n",
    "text_snippet = read_text[20000:23000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating Text\n",
    "\n",
    "There are many different ways that you could go about translating text within Python, but one of the easiest is the package `translation`. `translation` makes use of existing online translators. The module used to include a method for Google Translate, but the site no longer allows easy access. Bing is probably the most useful method for it.\n",
    "\n",
    "**Pros:**\n",
    "* Easy to set up\n",
    "* Runs quickly\n",
    "\n",
    "**Cons:**\n",
    "* Not always accurate\n",
    "* Internet connection needed\n",
    "* Language limitations\n",
    "\n",
    "The documentation (or lack there of): https://pypi.python.org/pypi/translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "translation.bing(text_snippet, dst = 'en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other alternatives for translating your text include:\n",
    "* `py-translate`\n",
    "    * Makes use of Google Translate\n",
    "    * Often return errors / gets blocked\n",
    "    * Can be used from the command line\n",
    "    * Documentation: https://pypi.python.org/pypi/py-translate\n",
    "\n",
    "\n",
    "* API calls to Google Translate\n",
    "    * Takes a little more set-up\n",
    "    * Can be customized a little bit more\n",
    "    * Can translate a LOT of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using py-translate\n",
    "from translate import translator\n",
    "\n",
    "# calling tranlator function, telling it that the \n",
    "translator('da', 'en',text_snippet[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polyglot\n",
    "\n",
    "Polyglot is \"a natural language pipeline that supports massive multilingual applications,\" in other words, it does a lot of stuff. It is a sort of one-stop-shop for many different functions that you may want to apply to you text, and supports many different languages. We are going to run through some of its functionalities.\n",
    "\n",
    "Docs: http://polyglot.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from polyglot.detect import Detector\n",
    "\n",
    "# create a detector object that contains read_text\n",
    "# and assigning it to DETECTED\n",
    "detected = Detector(read_text)\n",
    "\n",
    "# the .language method will return the language the most of\n",
    "# the text is made up of and the system is confident about\n",
    "print(detected.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sometimes there will be multiple languages within\n",
    "# the text, and you will want to see all of them\n",
    "for language in detected.languages:\n",
    "  print(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if you try to pass in a string that is too short\n",
    "# for the system to get a good read on, it will throw\n",
    "# an error, alerting you to this fact\n",
    "Detector(\"4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can override that with the optional argument 'quiet=True'\n",
    "print(Detector(\"4\", quiet=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# here are all of the languages supported for language detection\n",
    "from polyglot.utils import pretty_list\n",
    "print(pretty_list(Detector.supported_languages()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "\n",
    "Similar to what we saw with NLTK, Polyglot can break our text up into words and sentences. Polyglot has the advantage of spanning multiple languages, and thus is more likely to identify proper breakpoint in languages other than English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from polyglot.text import Text\n",
    "\n",
    "# creating a Text object that analyzes our text_snippet\n",
    "text = Text(text_snippet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Text also has a language instance variable\n",
    "print(text.language)\n",
    "\n",
    "# here, we are looking at text_snippet tokenized into words\n",
    "text.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now we are looking at text_snippet broken down into sentences\n",
    "text.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side Notes: Important Package Information\n",
    "\n",
    "Not all of the packages are downloaded for all functionalities for all languages in Polyglot. Instead of forcing you to download a lot of files in the beginning, the creators decided that it would be better for language extensions to be downloaded on an 'as-necessary' basis. You will occassionaly be told that you're lacking a package, and you will need to download it. You can either do that with the built-in downloader, or from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# staying within python\n",
    "from polyglot.downloader import downloader\n",
    "downloader.download(\"embeddings2.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# alternate command line method\n",
    "!polyglot download embeddings2.da pos2.da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, if you're working with a language and want to know what Polyglot lets you do with a language, it provides a `supported_tasks` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tasks available for english\n",
    "downloader.supported_tasks(lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tasks available for danish\n",
    "downloader.supported_tasks(lang=\"da\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part of Speech Tagging\n",
    "\n",
    "Polyglot supports POS tagging for several languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# languages that polyglot supports for part of speech tagging\n",
    "print(downloader.supported_languages_table(\"pos2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text.pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition\n",
    "\n",
    "Polyglot can tag names and groups them into three main categories:\n",
    "* Locations (Tag: I-LOC): cities, countries, regions, continents, neighborhoods, administrative divisions ...\n",
    "* Organizations (Tag: I-ORG): sports teams, newspapers, banks, universities, schools, non-profits, companies, ...\n",
    "* Persons (Tag: I-PER): politicians, scientists, artists, atheletes ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# languages that polyglot supports for part of speech tagging\n",
    "print(downloader.supported_languages_table(\"ner2\", 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!polyglot download ner2.da\n",
    "text.entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Features of Polyglot\n",
    "* Nearest Neighbors -- http://polyglot.readthedocs.io/en/latest/Embeddings.html\n",
    "* Morpheme Generation -- http://polyglot.readthedocs.io/en/latest/MorphologicalAnalysis.html\n",
    "* Sentiment Analysis -- http://polyglot.readthedocs.io/en/latest/Sentiment.html\n",
    "* Transliteration -- http://polyglot.readthedocs.io/en/latest/Transliteration.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Summary:\n",
    "\n",
    "#### Translation:\n",
    "* `translation.bing(your_string, dst = 'en')`\n",
    "\n",
    "#### Polyglot:\n",
    "* `<Detector>.language`\n",
    "* `<Detector>.languages`\n",
    "* `<Text>.language`\n",
    "* `<Text>.words`\n",
    "* `<Text>.sentences`\n",
    "* `<Text>.pos_tags`\n",
    "* `<Text>.entities`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing some more packages\n",
    "from datascience import *\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# analyzing our text with a Polyglot Text object\n",
    "whole_text = Text(read_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the language of our text\n",
    "print(whole_text.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# getting the part of speech tags for our corpus\n",
    "print(whole_text.pos_tags)\n",
    "words_and_poss = list(whole_text.pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# putting those word / part of speech pairs into a table\n",
    "wrd = Table(['Word', 'Part of Speech']).with_rows(words_and_poss)\n",
    "# grouping those by part of speech to get the most commonly occuring parts of speech\n",
    "df = wrd.group('Part of Speech').sort('count', descending=True).to_df()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting the counts for each part of speech using seaborn\n",
    "sns.barplot(x='Part of Speech', y='count', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# getting the most popular word for each part of speech type\n",
    "wrd_counts = wrd.group('Word').join('Word', wrd).sort('count', descending=True)\n",
    "wrd_counts.group(2, lambda x: x.item(0)).show(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# thats not very informative, so lets pull out the stop words\n",
    "# using a list from http://snowball.tartarus.org/algorithms/danish/stop.txt\n",
    "danish_stop_words = \"\"\"og,\n",
    "i,\n",
    "jeg,\n",
    "det,\n",
    "at,\n",
    "en,\n",
    "den,\n",
    "til,\n",
    "er,\n",
    "som,\n",
    "på,\n",
    "de,\n",
    "med,\n",
    "han,\n",
    "af,\n",
    "for,\n",
    "ikke,\n",
    "der,\n",
    "var,\n",
    "mig,\n",
    "sig,\n",
    "men,\n",
    "et,\n",
    "har,\n",
    "om,\n",
    "vi,\n",
    "min,\n",
    "havde,\n",
    "ham,\n",
    "hun,\n",
    "nu,\n",
    "over,\n",
    "da,\n",
    "fra,\n",
    "du,\n",
    "ud,\n",
    "sin,\n",
    "dem,\n",
    "os,\n",
    "op,\n",
    "man,\n",
    "hans,\n",
    "hvor,\n",
    "eller,\n",
    "hvad,\n",
    "skal,\n",
    "selv,\n",
    "her,\n",
    "alle,\n",
    "vil,\n",
    "blev,\n",
    "kunne,\n",
    "ind,\n",
    "når,\n",
    "være,\n",
    "dog,\n",
    "noget,\n",
    "ville,\n",
    "jo,\n",
    "deres,\n",
    "efter,\n",
    "ned,\n",
    "skulle,\n",
    "denne,\n",
    "end,\n",
    "dette,\n",
    "mit,\n",
    "også,\n",
    "under,\n",
    "have,\n",
    "dig,\n",
    "anden,\n",
    "hende,\n",
    "mine,\n",
    "alt,\n",
    "meget,\n",
    "sit,\n",
    "sine,\n",
    "vor,\n",
    "mod,\n",
    "disse,\n",
    "hvis,\n",
    "din,\n",
    "nogle,\n",
    "hos,\n",
    "blive,\n",
    "mange,\n",
    "ad,\n",
    "bliver,\n",
    "hendes,\n",
    "været,\n",
    "thi,\n",
    "jer,\n",
    "sådan\"\"\"\n",
    "splt = danish_stop_words.split(',\\n')\n",
    "print(splt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# determining which rows we need to change\n",
    "not_in_stop_words = [x not in danish_stop_words for x in wrd_counts['Word']]\n",
    "# most common words for each part of speech no longer including the stop words\n",
    "wrd_counts.where(not_in_stop_words).group(2, lambda x: x.item(0)).show(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# retrieving all of the named entities that Polyglot detected\n",
    "ner = str(whole_text.entities).split('I-')[1:]\n",
    "ner[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# splitting up the type and the name\n",
    "split_type = [x.split('([') for x in ner]\n",
    "split_type[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# making a table out of that\n",
    "entities = Table(['Type', 'Name']).with_rows(split_type)\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how many of each type of entity there are\n",
    "entities.group('Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# finding the most commonly occuring entities\n",
    "entities.group('Name').sort('count', descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# possibly the most common names of people\n",
    "entities.where('Type', 'PER').group('Name').sort('count', True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
