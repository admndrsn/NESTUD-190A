{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodes and Edges\n",
    "\n",
    "<b>Intent:</b> Taking a look at some of the things that you may need to use Python for in creating your network graphs.\n",
    "\n",
    "<b>Data from:</b> Gabriella's spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "from datascience import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing CSVs\n",
    "We are going to read in a csv that we downloaded from Google Sheets, using the `read_table` method from `datascience`. Below is a picture of what it originally looked like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Google Sheets.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graffiti = Table().read_table('WellonsG Graffiti mochicas in the Huaca Cao Viejo, El Brujo - Sheet1.csv')\n",
    "graffiti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# selecting out the columns that we want to turn into our node / edge pairs\n",
    "temple_and_code = graffiti.select('Temple', 'Code')\n",
    "temple_and_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to add a unique numerical ID to each of our items so that we can make our network graph. In order to do that, we will use a dictionary to keep track of what ID we assign to each different element. We are going to then define functions for adding elements to our dictionary and for getting the values that we assign to our keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we are going to count up from zero as we assign ID's to keys\n",
    "next_id = 0\n",
    "# initializing the dictionary that we will keep our key-value pairs in\n",
    "dictionary = {}\n",
    "\n",
    "# making a function for adding keys\n",
    "def add_to_dictionary(key):\n",
    "    global next_id\n",
    "    if key not in dictionary.keys():\n",
    "        dictionary[key] = next_id\n",
    "        next_id = next_id + 1\n",
    "\n",
    "# a function to get back values from the dictionary\n",
    "def get_id(key):\n",
    "    return dictionary[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note:</b> \\>>> (three greater than signs) means that that line is something we typed\n",
    "\n",
    "#### <font color='blue'> Step by step of what we just defined</font>\n",
    "\n",
    "\n",
    "```\n",
    ">>> add_to_dictionary('E')\n",
    "```\n",
    "\n",
    "When we add 'dog' to our dictionary, we assign a unique ID to it, and store those values together. Think of this process as the computer remembering it like:\n",
    "\n",
    "```\n",
    "'dog' = 0\n",
    "```\n",
    "We then want to be able to get back an ID if we pass in a key:\n",
    "\n",
    "```\n",
    ">>> get_id('E')\n",
    "0\n",
    "```\n",
    "This a repeatable process that will hold true as we continue to pass more things in.\n",
    "\n",
    "```\n",
    ">>> add_to_dictionary('D')\n",
    ">>> add_to_dictionary('E1')\n",
    ">>> add_to_dictionary('E2')\n",
    ">>> get_id('E2')\n",
    "3\n",
    ">>> get_id('D')\n",
    "1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# telling the computer, 'for each label in the labels\n",
    "# of our temple_and_code table repeat this process':\n",
    "for label in temple_and_code.labels:\n",
    "    # apply the add_to_dictionary function to each value in the column 'label' of this table\n",
    "    temple_and_code.apply(add_to_dictionary, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating columns for our node\n",
    "nodes_and_edges = temple_and_code.with_columns([\n",
    "        'Source', temple_and_code.apply(get_id, 'Temple'),\n",
    "        'Edge', temple_and_code.apply(get_id, 'Code')\n",
    "    ])\n",
    "nodes_and_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have ID's for our table, but now we want weights for them that represent the strength of the relationship. There are many different ways to quantify a relationship, but we will do it based of frequency of appearance in our table. We will use the `group` function to let us know how many times a Temple-Code combo appears in our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = temple_and_code.group(['Temple', 'Code'])\n",
    "weights.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then will use the `pandas` function `merge` to join the two tables together so that we have the weights with our node and edge info. Notice that we need to convert our tables to `pandas` dataframes so that they will work with the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with_weights = pd.merge(nodes_and_edges.to_df(), weights.to_df(), how='right',on=['Temple', 'Code'])\n",
    "# converting back to a datascience table\n",
    "with_weights = Table().from_df(with_weights)\n",
    "with_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# relabeling a column\n",
    "with_weights.relabel('count', 'Weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#if you would like to save the file as a csv, uncomment the following line (remove the #)\n",
    "#with_weights.to_csv('temple_nodes_edges.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example is a good demonstration on how we would convert a bunch of nodes and edges to unique ids. Using these conversions, we can also organize and weight our info in a manner that will help us in creating network graphs. However, for this example, we already had a spreadsheet provided; this spreadsheet already identified important nodes and edges. For most of you, this identification of important nodes and edges is the truly hard part of the data analysis. This is where all the previous information that has been taught in the modules comes into play. By utilizing tools such as regular expressions and NLTK, you should try and make relavant and specific queries that return you specific connections and correlations in your data sets/texts; these connections and correlations can eventually be transformed into nodes and edges! To review how this process works, let us look at a powerful example from a previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing relevant packages\n",
    "import re\n",
    "import codecs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk import tokenize\n",
    "from collections import Counter\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We are going to revisit our texts regarding Phillip II of Spain\n",
    "with codecs.open('Grand Strategy of Phillip II Text.txt', 'r',encoding = 'utf-8', errors='ignore') as f:\n",
    "    read_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's start off by getting a rough list of proper nouns with relative frequencies - this will help us narrow down people, places and their relations\n",
    "# capital 'W' mean NOT word characters (not letters or numbers)\n",
    "pp.pprint(Counter(re.findall('[A-Z]\\w+',read_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# making a table out of the above dictionary\n",
    "names_table = Table(['Name', 'Count'])\n",
    "names = Counter(re.findall('[A-Z]\\w+',read_text))\n",
    "for name in names.keys():\n",
    "    row = [name, names[name]]\n",
    "    names_table.append(row)\n",
    "names_table.sort('Count', descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with this simple, query, we see that we are able to gain important information regarding the most commonly occuring names/proper nouns in the work. If we do more analysis regarding these parts of the texts, we may be able to develop clear nodes and edges between relevant pieces of information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# while we're at it, let's get a list of all the years that appear in the text along with relative frequencies\n",
    "pp.pprint(Counter(re.findall('\\d{4}',read_text))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important aspects of data analysis is CREATIVITY! Analysis, especially of dense works, is never black-and-white. Rather, the person analyzing the data must think of meaningful queries and searches that will yield the most useful results. Take some time to brainstorm like a data scientist! Using a list of the most common years and proper nouns as a starting point, what are some more/resulting things that you feel you could analyze/query using our data analysis tools? THINK BIG! Often, people feel that their idea is too ambitious; however, most of the time, with enough work, we can get a computer to make that vision a reality. Share your ideas with the class!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to revisit an example from a previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's see the context surrouding the instances in the text where our most common name and most common country (besides Spain)\n",
    "#are mentioned in close proximity\n",
    "info_england = re.findall('[\\S\\s]{,45}England[\\S\\s]{,45}', read_text)\n",
    "info_england_parsed = []\n",
    "for elem in info_england:\n",
    "    if 'Philip' in elem:\n",
    "        info_england_parsed.append(elem)\n",
    "info_england_parsed\n",
    "#What are ways that you could improve this query?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's organize this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date_words = [re.findall('[A-Z][a-z]+',elem) + re.findall('\\d{4}', elem) for elem in info_england_parsed if re.search('[A-Z][a-z]+', elem)]\n",
    "date_words\n",
    "#Using this query, we can associate proper nouns with the year around which they were referenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Finally, let's make a dictionary that organizes this data! This dictionary is created in a similar manner to \n",
    "#the other dictionaries above. In this one, our \"keys\" are years. The keys are associated with words that are \n",
    "#mentioned in close proximity to that year.\n",
    "word_date_dict = {}\n",
    "for x in range(1500,1600):\n",
    "    for elem in date_words:\n",
    "        if str(x) in elem:\n",
    "            word_date_dict[str(x)] = elem\n",
    "word_date_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Look what we were able to do with only a few, simple, rough queries! Creating similar sets of organized info will help greatly in finding node/edge relationships. What are some relationships that you can pick out from the above information?\n",
    "\n",
    "Keep in mind that you are not limited to only names and years! By analyzing the patterns in the text, you can pull out relationships between names, places, events, artifacts, etc. These relationships will help you greatly in creating your network structure.\n",
    "\n",
    "Experiment with all that you've learned! Ask questions if you have them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
